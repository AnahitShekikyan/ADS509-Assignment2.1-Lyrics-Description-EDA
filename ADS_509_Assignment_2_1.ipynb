{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOyQ+buxHu0OHq2RjGoBCNQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnahitShekikyan/ADS509-Assignment2.1-Lyrics-Description-EDA/blob/main/ADS_509_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 2.1: Tokenization, Normalization, and Descriptive Statistics**\n",
        "\n",
        "#### **Course:** ADS 509, Applied Large Language Models for Data Science\n",
        "\n",
        "#### **Name:** Anna Shekikyan\n",
        "\n",
        "#### **Date:** 09/15/2025\n",
        "\n",
        "#### **GitHub:** https://github.com/AnahitShekikyan/ADS509-Assignment2.1-Lyrics-Description-EDA\n",
        "\n",
        "#### **ipynb:** https://colab.research.google.com/drive/18TJd7apglDbLVLzP4xsM9poYBFqsliM6?usp=sharing"
      ],
      "metadata": {
        "id": "x2Bgxr6oLLUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8PN20kEK3zP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "! pip install emoji\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "! pip install nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional imports statements\n",
        "import json\n",
        "import unicodedata\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "Ikl3hx0DMWUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change `data_location` to the location of the folder on your machine.\n",
        "data_location = \"/users/chandler/dropbox/teaching/repos/ads-tm-api-scrape/\"\n",
        "\n",
        "# These subfolders should still work if you correctly stored the\n",
        "# data from the Module 1 assignment\n",
        "twitter_folder = \"twitter/\"\n",
        "lyrics_folder = \"lyrics/\""
      ],
      "metadata": {
        "id": "bDYekQE6M7QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
        "    \"\"\"\n",
        "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
        "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity),\n",
        "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
        "        of unique tokens, lexical diversity, and number of characters.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Fill in the correct values here.\n",
        "    num_tokens = len(tokens)\n",
        "    num_unique_tokens =  len(set(tokens))\n",
        "    lexical_diversity = (num_unique_tokens / num_tokens) if num_tokens > 0 else 0.0\n",
        "    num_characters = sum(len(t) for t in tokens)\n",
        "\n",
        "    if verbose :\n",
        "        print(f\"There are {num_tokens} tokens in the data.\")\n",
        "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
        "        print(f\"There are {num_characters} characters in the data.\")\n",
        "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
        "\n",
        "        # print the five most common tokens\n",
        "\n",
        "    return([num_tokens, num_unique_tokens,\n",
        "            lexical_diversity,\n",
        "            num_characters])"
      ],
      "metadata": {
        "id": "VPn4OLJdagyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
        "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
        "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
        "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
        "assert(descriptive_stats(text, verbose=False)[3] == 55)"
      ],
      "metadata": {
        "id": "QAScKUGIan4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q:` Why is it beneficial to use assertion statements in your code?\n",
        "\n",
        "`A:` Assertions catch violations of assumptions early, right where they occur. They serve as lightweight, executable checks for invariants and expected outputs (like your unit-test style asserts), making bugs easier to locate and preventing silent failures that would otherwise propagate."
      ],
      "metadata": {
        "id": "7Wt6lywecKfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Input\n",
        "\n",
        "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well.\n",
        "\n",
        "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist.\n"
      ],
      "metadata": {
        "id": "18bmiRmKcwE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Input, Setup\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# The folder that directly contains both 'lyrics' and 'twitter'\n",
        "data_base = Path(\"/content/drive/MyDrive/M1 Assignment Data/M1 Assignment Data/M1 Results\").resolve()\n",
        "\n",
        "print(\"Using data base:\", data_base)\n",
        "print(\"lyrics exists:\", (data_base / \"lyrics\").is_dir(), \"| twitter exists:\", (data_base / \"twitter\").is_dir())"
      ],
      "metadata": {
        "id": "_7USum_qX2xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the lyrics data\n",
        "\n",
        "lyrics_dir = (data_base / \"lyrics\").resolve()\n",
        "lyrics_data = defaultdict(dict)   # {artist: {title: full_text}}\n",
        "\n",
        "def first_nonempty_line(text: str, fallback: str) -> str:\n",
        "    for ln in text.splitlines():\n",
        "        s = ln.strip()\n",
        "        if s:\n",
        "            return s\n",
        "    return fallback\n",
        "\n",
        "if lyrics_dir.is_dir():\n",
        "    for f in lyrics_dir.rglob(\"*\"):\n",
        "        if f.is_file() and f.suffix.lower() in {\".txt\", \".md\", \".lyr\"}:\n",
        "            try:\n",
        "                txt = f.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
        "            except Exception:\n",
        "                txt = f.read_text(encoding=\"latin1\", errors=\"ignore\").strip()\n",
        "\n",
        "            artist = f.parent.name if f.parent != lyrics_dir else (\n",
        "                f.stem.split(\"_\", 1)[0].lower() if \"_\" in f.stem else \"unknown\"\n",
        "            )\n",
        "            title_fallback = f.stem.split(\"_\", 1)[1].replace(\"_\", \" \").strip() if \"_\" in f.stem else f.stem\n",
        "            title = first_nonempty_line(txt, title_fallback)\n",
        "\n",
        "            lyrics_data[artist][title] = txt\n",
        "\n",
        "print(f\"Loaded lyrics — artists: {len(lyrics_data)} | songs: {sum(len(v) for v in lyrics_data.values())}\")"
      ],
      "metadata": {
        "id": "flsAndw9d76b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peek a few\n",
        "shown = 0\n",
        "for a, songs in lyrics_data.items():\n",
        "    for t in songs:\n",
        "        print(f\"- {a}: {t[:60]}\")\n",
        "        shown += 1\n",
        "        if shown >= 3: break\n",
        "    if shown >= 3: break"
      ],
      "metadata": {
        "id": "ZhJEnJn9YnCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the twitter data\n",
        "twitter_dir = (data_base / \"twitter\").resolve()\n",
        "twitter_desc = defaultdict(list)  # {artist: [description, ...]}\n",
        "\n",
        "def norm(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "\n",
        "def candidate_files(root: Path):\n",
        "    for f in root.rglob(\"*\"):\n",
        "        if f.is_file() and (\"followers\" in f.name.lower()) and f.suffix.lower() in {\".csv\", \".tsv\", \".txt\"}:\n",
        "            yield f\n",
        "\n",
        "def read_followers(path: Path):\n",
        "    # TSV with header\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=\"\\t\", engine=\"python\", encoding=\"utf-8\", on_bad_lines=\"skip\", dtype=str)\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(path, sep=\"\\t\", engine=\"python\", encoding=\"latin1\", on_bad_lines=\"skip\", dtype=str)\n",
        "    except Exception:\n",
        "        df = None\n",
        "\n",
        "    if df is not None:\n",
        "        cols = [c for c in df.columns if c.lower() == \"description\"]\n",
        "        if cols:\n",
        "            vals = df[cols[0]].dropna().astype(str).map(norm)\n",
        "            return [v for v in vals if v]\n",
        "\n",
        "    # TSV without header → last column\n",
        "    try:\n",
        "        df_noh = pd.read_csv(path, sep=\"\\t\", engine=\"python\", header=None, encoding=\"utf-8\",\n",
        "                             on_bad_lines=\"skip\", dtype=str)\n",
        "    except UnicodeDecodeError:\n",
        "        df_noh = pd.read_csv(path, sep=\"\\t\", engine=\"python\", header=None, encoding=\"latin1\",\n",
        "                             on_bad_lines=\"skip\", dtype=str)\n",
        "    except Exception:\n",
        "        df_noh = None\n",
        "\n",
        "    if df_noh is not None and df_noh.shape[1] >= 2:\n",
        "        last = df_noh.columns[-1]\n",
        "        vals = df_noh[last].dropna().astype(str).map(norm)\n",
        "        vals = [v for v in vals if v and v.lower() != \"description\"]\n",
        "        if vals:\n",
        "            return vals\n",
        "\n",
        "    # CSV/unknown delimiter with header (pandas sniff)\n",
        "    try:\n",
        "        df_any = pd.read_csv(path, sep=None, engine=\"python\", encoding=\"utf-8\",\n",
        "                             on_bad_lines=\"skip\", dtype=str)\n",
        "    except UnicodeDecodeError:\n",
        "        df_any = pd.read_csv(path, sep=None, engine=\"python\", encoding=\"latin1\",\n",
        "                             on_bad_lines=\"skip\", dtype=str)\n",
        "    except Exception:\n",
        "        df_any = None\n",
        "\n",
        "    if df_any is not None:\n",
        "        cols = [c for c in df_any.columns if c.lower() == \"description\"]\n",
        "        if cols:\n",
        "            vals = df_any[cols[0]].dropna().astype(str).map(norm)\n",
        "            return [v for v in vals if v]\n",
        "\n",
        "    return []\n",
        "\n",
        "if twitter_dir.is_dir():\n",
        "    for f in candidate_files(twitter_dir):\n",
        "        artist = f.parent.name if f.parent != twitter_dir else (\n",
        "            f.stem.split(\"_\", 1)[0].lower() if \"_\" in f.stem else \"unknown\"\n",
        "        )\n",
        "        descs = read_followers(f)\n",
        "        if descs:\n",
        "            twitter_desc[artist].extend(descs)\n",
        "\n",
        "    # deduplicate per artist\n",
        "    for a, ds in list(twitter_desc.items()):\n",
        "        seen, uniq = set(), []\n",
        "        for s in ds:\n",
        "            if s not in seen:\n",
        "                seen.add(s); uniq.append(s)\n",
        "        twitter_desc[a] = uniq\n",
        "\n",
        "print(f\"Collected twitter descriptions — artists: {len(twitter_desc)} | total unique: {sum(len(v) for v in twitter_desc.values())}\")"
      ],
      "metadata": {
        "id": "86e1ekpXOVDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek into a few Twitter bios per artist\n",
        "for artist, descs in twitter_desc.items():\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Artist: {artist} | total bios: {len(descs)}\")\n",
        "    for d in descs[:5]:   # show first 5 bios\n",
        "        print(\" -\", d[:120])  # truncate to 120 chars for readability\n",
        "    print()"
      ],
      "metadata": {
        "id": "4XYECeCEOU6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = set(punctuation) # speeds up comparison"
      ],
      "metadata": {
        "id": "HaAR58ZT9m4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "939c272a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean twitter data\n",
        "URL_RE   = re.compile(r'https?://\\S+|www\\.\\S+', re.I)\n",
        "EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', re.I)\n",
        "NUM_RE   = re.compile(r'\\b\\d+(?:[\\.,]\\d+)?\\b')\n",
        "TOKEN_RE = re.compile(r\"(?:[#@]?\\w[\\w']*)\", re.UNICODE)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sw_set = set(stopwords.words('english'))\n",
        "\n",
        "def deaccent(s: str) -> str:\n",
        "    return unicodedata.normalize('NFKD', s).encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "def pos2wn(tag: str):\n",
        "    if tag.startswith('J'): return wordnet.ADJ\n",
        "    if tag.startswith('V'): return wordnet.VERB\n",
        "    if tag.startswith('N'): return wordnet.NOUN\n",
        "    if tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def clean_text_tokens(text: str, keep_hashtags=True, keep_mentions=False, drop_numbers=True):\n",
        "    import nltk\n",
        "    t = deaccent(str(text).lower())\n",
        "    t = URL_RE.sub(' ', t)\n",
        "    t = EMAIL_RE.sub(' ', t)\n",
        "    if drop_numbers:\n",
        "        t = NUM_RE.sub(' ', t)\n",
        "\n",
        "    raw = TOKEN_RE.findall(t)\n",
        "\n",
        "    def keep(tok):\n",
        "        if tok.startswith('#') and not keep_hashtags: return False\n",
        "        if tok.startswith('@') and not keep_mentions: return False\n",
        "        return True\n",
        "\n",
        "    toks = [tok.strip(''.join(punctuation)) for tok in raw if tok and keep(tok)]\n",
        "    toks = [tok for tok in toks if tok and tok not in sw_set and not emoji.is_emoji(tok)]\n",
        "\n",
        "    if toks:\n",
        "        pos = nltk.pos_tag(toks)\n",
        "        toks = [lemmatizer.lemmatize(tok, pos2wn(p)) for tok, p in pos]\n",
        "    return toks\n",
        "\n",
        "# Clean twitter data\n",
        "clean_twitter = {}\n",
        "for artist, bios in twitter_desc.items():\n",
        "    combined = \" \".join(bios)\n",
        "    clean_twitter[artist] = clean_text_tokens(combined, keep_hashtags=True, keep_mentions=False, drop_numbers=True)"
      ],
      "metadata": {
        "id": "TR8LR9bnhXGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peek\n",
        "for a in clean_twitter:\n",
        "    print(\"=\"*60, f\"\\n{a} — TWITTER\")\n",
        "    _ = descriptive_stats(clean_twitter[a], num_tokens=5, verbose=True)"
      ],
      "metadata": {
        "id": "jbA7zgTKjoBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean lyrics data\n",
        "clean_lyrics = {}\n",
        "for artist, songs in lyrics_data.items():\n",
        "    combined = \" \".join(map(str, songs.values()))\n",
        "    clean_lyrics[artist] = clean_text_tokens(\n",
        "        combined,\n",
        "        keep_hashtags=False,  # usually irrelevant in lyrics\n",
        "        keep_mentions=False,\n",
        "        drop_numbers=False    # numbers can be meaningful in lyrics\n",
        "    )\n",
        "\n",
        "print(\"Artists (lyrics cleaned): \", list(clean_lyrics.keys())[:5])\n"
      ],
      "metadata": {
        "id": "ecCN0GVf92BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Descriptive Statistics\n",
        "\n",
        "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls)."
      ],
      "metadata": {
        "id": "FkQ6kZ0E-D9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calls to descriptive_stats here\n",
        "artists = list(clean_lyrics.keys())\n",
        "for artist in artists[:2]:\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{artist} — LYRICS\")\n",
        "    _ = descriptive_stats(clean_lyrics[artist], num_tokens=5, verbose=True)\n",
        "\n",
        "    if artist in clean_twitter:\n",
        "        print(\"-\"*60)\n",
        "        print(f\"{artist} — TWITTER\")\n",
        "        _ = descriptive_stats(clean_twitter[artist], num_tokens=5, verbose=True)"
      ],
      "metadata": {
        "id": "qSe0xiDv-AIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q:` How do you think the \"top 5 words\" would be different if we left stopwords in the data?\n",
        "\n",
        "`A:` If stopwords were left in, the most frequent words would likely be function words such as the, and, to, of, and in. These words occur at very high frequency but contribute little semantic meaning. Their presence would mask the more distinctive and meaningful terms in both the lyrics and the Twitter bios.\n",
        "\n",
        "---\n",
        "\n",
        "`Q:` What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs?\n",
        "\n",
        "`A:` Lyrics generally display higher lexical diversity than Twitter bios because songs contain more creative and varied language, while bios are short and often repetitive. The results align with this expectation: Robyn’s lyrics exhibited greater lexical diversity (0.154) compared to Cher’s lyrics (0.091), and Cher’s Twitter bios reflected a similar level of lexical diversity (0.091). These results indicate that both text type and artist style influence lexical diversity, with Robyn’s songs demonstrating more linguistic variety than Cher’s."
      ],
      "metadata": {
        "id": "nBK9Dum2-UTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specialty Statistics\n",
        "\n",
        "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
        "\n",
        "1. Ten most common emojis by artist in the twitter descriptions.\n",
        "1. Ten most common hashtags by artist in the twitter descriptions.\n",
        "1. Five most common words in song titles by artist.\n",
        "1. For each artist, a histogram of song lengths (in terms of number of tokens)\n",
        "\n",
        "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
      ],
      "metadata": {
        "id": "3-jvrUms-cil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert(emoji.is_emoji(\"❤️\"))\n",
        "assert(not emoji.is_emoji(\":-)\"))"
      ],
      "metadata": {
        "id": "yMkpPP1A-PH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emojis 😁\n",
        "\n",
        "What are the ten most common emojis by artist in the twitter descriptions?"
      ],
      "metadata": {
        "id": "U6XRubRn-lvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_counts_by_artist = {}\n",
        "for artist, descs in twitter_desc.items():\n",
        "    # examine raw text for emojis\n",
        "    chars = []\n",
        "    for d in descs:\n",
        "        chars.extend(list(str(d)))\n",
        "    emjs = [c for c in chars if emoji.is_emoji(c)]\n",
        "    emoji_counts_by_artist[artist] = Counter(emjs).most_common(10)\n",
        "\n",
        "emoji_counts_by_artist"
      ],
      "metadata": {
        "id": "OUmoSg5q-hka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hashtags\n",
        "\n",
        "What are the ten most common hashtags by artist in the twitter descriptions?"
      ],
      "metadata": {
        "id": "ygbjeKXi-v3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_counts_by_artist = {}\n",
        "for artist, descs in twitter_desc.items():\n",
        "    toks = []\n",
        "    for d in descs:\n",
        "        toks.extend(re.split(r\"\\s+\", str(d).strip().lower()))\n",
        "    hashtags = [t.strip(\"\".join(punctuation)) for t in toks if t.startswith(\"#\") and len(t) > 1]\n",
        "    hashtags = [h for h in hashtags if h]  # clean empties\n",
        "    hashtag_counts_by_artist[artist] = Counter(hashtags).most_common(10)\n",
        "\n",
        "for artist, tags in hashtag_counts_by_artist.items():\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{artist} — Top 10 Hashtags\")\n",
        "    for h, c in tags:\n",
        "        print(f\"{h} : {c}\")\n"
      ],
      "metadata": {
        "id": "K8c0kBVN-rYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Song Titles\n",
        "\n",
        "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
      ],
      "metadata": {
        "id": "6saHKLUC-4C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def title_tokens(title: str):\n",
        "    # reuse your cleaner; keep numbers (can be meaningful in titles)\n",
        "    return clean_text_tokens(title, keep_hashtags=False, keep_mentions=False, drop_numbers=False)\n",
        "\n",
        "title_words_by_artist = {}\n",
        "for artist, songs in lyrics_data.items():\n",
        "    toks = []\n",
        "    for title in songs.keys():\n",
        "        toks.extend(title_tokens(title))\n",
        "    title_words_by_artist[artist] = Counter(toks).most_common(5)\n",
        "\n",
        "# display (dict + readable printout)\n",
        "title_words_by_artist\n",
        "\n",
        "for artist, items in title_words_by_artist.items():\n",
        "    print(\"=\"*40)\n",
        "    print(f\"{artist} — Top 5 Title Words\")\n",
        "    for w, c in items:\n",
        "        print(f\"{w} : {c}\")"
      ],
      "metadata": {
        "id": "01G015M_-4qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Song Lengths\n",
        "\n",
        "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out.\n"
      ],
      "metadata": {
        "id": "nvI7Ivzr_BKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_replicates = 1000\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
        "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
        "})\n",
        "\n",
        "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
      ],
      "metadata": {
        "id": "dQDAnAF__Bue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting.\n",
        "\n",
        "`Q:` What does the regular expression `'\\s+'` match on?\n",
        "\n",
        "`A:` The pattern \\s+ matches one or more whitespace characters, including spaces, tabs, and newlines. It is useful for collapsing multiple forms of whitespace into a single delimiter when splitting text into tokens.\n"
      ],
      "metadata": {
        "id": "jH1coPNy_J7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collapse_whitespace = re.compile(r'\\s+')\n",
        "\n",
        "def tokenize_lyrics(lyric) :\n",
        "    \"\"\"strip and split on whitespace\"\"\"\n",
        "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
      ],
      "metadata": {
        "id": "qU9lSZpY_KYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lyric length comparison chart here.\n",
        "\n",
        "length_rows = []\n",
        "for artist, songs in lyrics_data.items():\n",
        "    for title, text in songs.items():\n",
        "        toks = tokenize_lyrics(text.strip())\n",
        "        length_rows.append({\"artist\": artist, \"length\": len(toks)})\n",
        "\n",
        "length_df = pd.DataFrame(length_rows)\n",
        "\n",
        "if not length_df.empty:\n",
        "    length_df.groupby(\"artist\")[\"length\"].plot(\n",
        "        kind=\"hist\", density=True, alpha=0.5, legend=True\n",
        "    )\n",
        "    plt.xlabel(\"Tokens per song\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.title(\"Song Length Distributions by Artist\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KHyHFmeW_RfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_c6ytK_LP8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}